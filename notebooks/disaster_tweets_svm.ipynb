{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/TheZarif/disaster-tweets/raw/main/nlp-getting-started.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X5iT3CNy4jp",
        "outputId": "84af195c-32bf-4ec6-fda5-44d75f8dae2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-29 05:36:41--  https://github.com/TheZarif/disaster-tweets/raw/main/nlp-getting-started.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/TheZarif/disaster-tweets/main/nlp-getting-started.zip [following]\n",
            "--2023-11-29 05:36:41--  https://raw.githubusercontent.com/TheZarif/disaster-tweets/main/nlp-getting-started.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp-getting-started.zip’\n",
            "\n",
            "nlp-getting-started 100%[===================>] 593.11K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-11-29 05:36:41 (10.1 MB/s) - ‘nlp-getting-started.zip’ saved [607343/607343]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nlp-getting-started.zip -d data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db3fTTS_nxyD",
        "outputId": "a1eafc91-f5f7-4d74-847b-c3d163a4690c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nlp-getting-started.zip\n",
            "  inflating: data/sample_submission.csv  \n",
            "  inflating: data/test.csv           \n",
            "  inflating: data/train.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"/content/data/train.csv\")\n",
        "df_test = pd.read_csv(\"/content/data/test.csv\")\n",
        "\n",
        "print('Training Set Shape = {}'.format(df_train.shape))\n",
        "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
        "print('Test Set Shape = {}'.format(df_test.shape))\n",
        "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT9YK6zBoyJ6",
        "outputId": "4b441b62-4b32-4cd7-e210-13faece569a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Shape = (7613, 5)\n",
            "Training Set Memory Usage = 0.29 MB\n",
            "Test Set Shape = (3263, 4)\n",
            "Test Set Memory Usage = 0.10 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\n",
        "df_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n",
        "\n",
        "print(\"Train Length Stat\")\n",
        "print(df_train[\"length\"].describe())\n",
        "print()\n",
        "\n",
        "print(\"Test Length Stat\")\n",
        "print(df_test[\"length\"].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkOCWn51pUsk",
        "outputId": "922ccaa3-6c7a-4125-b89c-7cc92f1606f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Length Stat\n",
            "count    7613.000000\n",
            "mean      101.037436\n",
            "std        33.781325\n",
            "min         7.000000\n",
            "25%        78.000000\n",
            "50%       107.000000\n",
            "75%       133.000000\n",
            "max       157.000000\n",
            "Name: length, dtype: float64\n",
            "\n",
            "Test Length Stat\n",
            "count    3263.000000\n",
            "mean      102.108183\n",
            "std        33.972158\n",
            "min         5.000000\n",
            "25%        78.000000\n",
            "50%       109.000000\n",
            "75%       134.000000\n",
            "max       151.000000\n",
            "Name: length, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TRAINING_EXAMPLES = df_train.shape[0]\n",
        "TRAIN_SPLIT = 0.8\n",
        "VAL_SPLIT = 0.2\n",
        "EPOCHS = 2"
      ],
      "metadata": {
        "id": "xiTb_41AsJJX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_train[\"text\"]\n",
        "y = df_train[\"target\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n",
        "\n",
        "X_test = df_test[\"text\"]"
      ],
      "metadata": {
        "id": "2B_vrnnesVdG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    preprocessed_text = []\n",
        "    for t in text.split():\n",
        "        if len(t) > 1:\n",
        "            t = '@user' if t[0] == '@' and t.count('@') == 1 else t\n",
        "            t = 'http' if t.startswith('http') else t\n",
        "        preprocessed_text.append(t)\n",
        "    return ' '.join(preprocessed_text)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_val = X_val.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)"
      ],
      "metadata": {
        "id": "3B5bwB3bGg7F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define a range of candidate max_features values\n",
        "candidate_max_features = [1000, 5000, 10000, 20000]\n",
        "\n",
        "# Create a parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'vectorizer__max_features': candidate_max_features,\n",
        "    'classifier__kernel': ['linear', 'rbf'],\n",
        "    'classifier__C': [0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Create a pipeline with TF-IDF vectorizer and SVM classifier\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', SVC())\n",
        "])\n",
        "\n",
        "# Create GridSearchCV with the parameter grid and pipeline\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV on your training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Make predictions on the validation set using the best estimator\n",
        "y_pred = best_estimator.predict(X_val)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy on Validation Set: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFlRfUPtlpG5",
        "outputId": "8801b605-3b92-4738-aaeb-8187fe607d84"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__C': 1, 'classifier__kernel': 'rbf', 'vectorizer__max_features': 10000}\n",
            "Accuracy on Validation Set: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best estimator for predictions\n",
        "y_pred = grid_search.best_estimator_.predict(X_val)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy on Validation Set: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBXfDjwZqBIH",
        "outputId": "5f52ff8c-81a6-49d3-bd9c-2e5251c905fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation Set: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = grid_search.best_estimator_.predict(X_test)"
      ],
      "metadata": {
        "id": "4zXeT2gsqSn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['target'] = test_pred"
      ],
      "metadata": {
        "id": "KHDMJZudqkBp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT6hChqQqsgK",
        "outputId": "6054f745-72c7-41af-f34c-a720c64ee7e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[['id', 'target']].to_csv('/content/drive/My Drive/disaster-tweets/svm.csv', index=False)"
      ],
      "metadata": {
        "id": "QHB54NMoqxQC"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}